import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample dataset
data = [
    ("hello", "bonjour"),
    ("how are you", "comment ça va"),
    ("I am fine", "je vais bien"),
    ("what is your name", "comment tu t'appelles"),
    ("my name is", "je m'appelle"),
    ("thank you", "merci"),
    ("goodbye", "au revoir")
]

# (a) Data Preprocessing
def preprocess_data(data):
    # Split English and French sentences
    eng_texts = [pair[0] for pair in data]
    fra_texts = ['<start> ' + pair[1] + ' <end>' for pair in data]

    # Create tokenizers
    eng_tokenizer = Tokenizer()
    fra_tokenizer = Tokenizer()

    # Fit tokenizers
    eng_tokenizer.fit_on_texts(eng_texts)
    fra_tokenizer.fit_on_texts(fra_texts)

    # Convert texts to sequences
    eng_sequences = eng_tokenizer.texts_to_sequences(eng_texts)
    fra_sequences = fra_tokenizer.texts_to_sequences(fra_texts)

    # Find maximum lengths
    max_eng_len = max(len(seq) for seq in eng_sequences)
    max_fra_len = max(len(seq) for seq in fra_sequences)

    # Pad sequences
    eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')
    fra_padded = pad_sequences(fra_sequences, maxlen=max_fra_len, padding='post')

    return eng_padded, fra_padded, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len

# Process the data
eng_data, fra_data, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len = preprocess_data(data)

# Get vocabulary sizes
eng_vocab_size = len(eng_tokenizer.word_index) + 1
fra_vocab_size = len(fra_tokenizer.word_index) + 1

# Print shapes for debugging
print("Input shapes:")
print(f"English data shape: {eng_data.shape}")
print(f"French data shape: {fra_data.shape}")

# (b) Build Seq2Seq Model
def build_model(input_vocab, output_vocab, input_length, output_length):
    # Encoder
    encoder_inputs = Input(shape=(input_length,))
    encoder_embedding = Embedding(input_vocab, 50)(encoder_inputs)
    encoder = LSTM(100, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_embedding)
    encoder_states = [state_h, state_c]

    # Decoder
    decoder_inputs = Input(shape=(output_length,))
    decoder_embedding = Embedding(output_vocab, 50)(decoder_inputs)
    decoder_lstm = LSTM(100, return_sequences=True)
    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)
    decoder_dense = Dense(output_vocab, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    # Create model
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model

# (c) Prepare Data for Training
decoder_input_data = fra_data[:, :-1]  # Remove last token
decoder_target_data = fra_data[:, 1:]  # Remove first token

# Print shapes for debugging
print("\nTraining data shapes:")
print(f"Decoder input shape: {decoder_input_data.shape}")
print(f"Decoder target shape: {decoder_target_data.shape}")

# (d) Train the Model
model = build_model(
    eng_vocab_size,
    fra_vocab_size,
    max_eng_len,
    max_fra_len - 1
)

# Compile model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
history = model.fit(
    [eng_data, decoder_input_data],
    decoder_target_data,
    batch_size=2,
    epochs=100,
    validation_split=0.2
)

# (e) Inference Setup
class Translator:
    def init(self, model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len):
        self.model = model
        self.eng_tokenizer = eng_tokenizer
        self.fra_tokenizer = fra_tokenizer
        self.max_eng_len = max_eng_len
        self.max_fra_len = max_fra_len
        self.fra_index_word = {v: k for k, v in fra_tokenizer.word_index.items()}

    def translate(self, text):
        # Tokenize input text
        sequence = self.eng_tokenizer.texts_to_sequences([text])
        padded = pad_sequences(sequence, maxlen=self.max_eng_len, padding='post')

        # Initialize target sequence
        target_seq = np.zeros((1, self.max_fra_len - 1))

        # Generate translation
        prediction = self.model.predict([padded, target_seq])

        # Convert prediction to text
        output_sequence = np.argmax(prediction[0], axis=1)
        translated_text = []

        for idx in output_sequence:
            if idx != 0:
                word = self.fra_index_word.get(idx, '')
                if word and word not in ['<start>', '<end>']:
                    translated_text.append(word)

        return ' '.join(translated_text)

# (f) Translate New Sentences
translator = Translator(model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len)

# Test translations
test_sentences = [
    "hello",
    "thank you",
    "your name",
    "how are you"
]

print("\nTranslations:")
for sentence in test_sentences:
    translation = translator.translate(sentence)
    print(f"English: {sentence}")
    print(f"French: {translation}\n")
Input shapes:
English data shape: (7, 4)
French data shape: (7, 5)

Training data shapes:
Decoder input shape: (7, 4)
Decoder target shape: (7, 4)
Epoch 1/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 4s 231ms/step - accuracy: 0.0406 - loss: 2.7729 - val_accuracy: 0.3750 - val_loss: 2.7586
Epoch 2/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.3219 - loss: 2.7514 - val_accuracy: 0.2500 - val_loss: 2.7443
Epoch 3/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.2500 - loss: 2.7368 - val_accuracy: 0.2500 - val_loss: 2.7285
Epoch 4/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.2500 - loss: 2.7111 - val_accuracy: 0.2500 - val_loss: 2.7037
Epoch 5/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2500 - loss: 2.6876 - val_accuracy: 0.2500 - val_loss: 2.6722
Epoch 6/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.2500 - loss: 2.6443 - val_accuracy: 0.2500 - val_loss: 2.6226
Epoch 7/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.2500 - loss: 2.5796 - val_accuracy: 0.2500 - val_loss: 2.5474
Epoch 8/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2500 - loss: 2.5266 - val_accuracy: 0.2500 - val_loss: 2.4433
Epoch 9/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 0.2500 - loss: 2.4198 - val_accuracy: 0.2500 - val_loss: 2.3016
Epoch 10/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.2500 - loss: 2.1904 - val_accuracy: 0.2500 - val_loss: 2.1952
Epoch 11/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2500 - loss: 2.0819 - val_accuracy: 0.2500 - val_loss: 2.1944
Epoch 12/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2500 - loss: 2.1462 - val_accuracy: 0.2500 - val_loss: 2.1550
Epoch 13/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.2500 - loss: 2.2184 - val_accuracy: 0.2500 - val_loss: 2.1071
Epoch 14/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2500 - loss: 1.9860 - val_accuracy: 0.5000 - val_loss: 2.0737
Epoch 15/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.3469 - loss: 2.0707 - val_accuracy: 0.5000 - val_loss: 2.0754
Epoch 16/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.2656 - loss: 1.9656 - val_accuracy: 0.5000 - val_loss: 2.0854
Epoch 17/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.2188 - loss: 2.0231 - val_accuracy: 0.5000 - val_loss: 2.1109
Epoch 18/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.2906 - loss: 1.9737 - val_accuracy: 0.5000 - val_loss: 2.1491
Epoch 19/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.4031 - loss: 1.8527 - val_accuracy: 0.5000 - val_loss: 2.2063
Epoch 20/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.4656 - loss: 1.6372 - val_accuracy: 0.6250 - val_loss: 2.2825
Epoch 21/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.3719 - loss: 1.8147 - val_accuracy: 0.5000 - val_loss: 2.3647
Epoch 22/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5063 - loss: 1.5395 - val_accuracy: 0.5000 - val_loss: 2.4349
Epoch 23/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.4437 - loss: 1.6187 - val_accuracy: 0.3750 - val_loss: 2.5034
Epoch 24/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5719 - loss: 1.5058 - val_accuracy: 0.3750 - val_loss: 2.5845
Epoch 25/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.6219 - loss: 1.5866 - val_accuracy: 0.3750 - val_loss: 2.6838
Epoch 26/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5906 - loss: 1.6022 - val_accuracy: 0.3750 - val_loss: 2.8077
Epoch 27/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5813 - loss: 1.5013 - val_accuracy: 0.3750 - val_loss: 2.9459
Epoch 28/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.5813 - loss: 1.2834 - val_accuracy: 0.3750 - val_loss: 3.0821
Epoch 29/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.6438 - loss: 1.0418 - val_accuracy: 0.3750 - val_loss: 3.2310
Epoch 30/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5500 - loss: 1.2115 - val_accuracy: 0.5000 - val_loss: 3.3703
Epoch 31/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.6938 - loss: 1.0518 - val_accuracy: 0.5000 - val_loss: 3.5059
Epoch 32/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.7344 - loss: 0.9827 - val_accuracy: 0.5000 - val_loss: 3.6055
Epoch 33/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.7969 - loss: 0.8380 - val_accuracy: 0.5000 - val_loss: 3.6899
Epoch 34/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.8062 - loss: 0.8066 - val_accuracy: 0.5000 - val_loss: 3.7726
Epoch 35/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.8469 - loss: 0.8005 - val_accuracy: 0.5000 - val_loss: 3.8472
Epoch 36/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 1.0000 - loss: 0.6103 - val_accuracy: 0.5000 - val_loss: 3.9035
Epoch 37/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.5114 - val_accuracy: 0.5000 - val_loss: 3.9705
Epoch 38/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.4965 - val_accuracy: 0.5000 - val_loss: 4.0262
Epoch 39/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.4610 - val_accuracy: 0.5000 - val_loss: 4.0643
Epoch 40/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.4193 - val_accuracy: 0.5000 - val_loss: 4.1100
Epoch 41/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.3341 - val_accuracy: 0.5000 - val_loss: 4.1590
Epoch 42/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.3083 - val_accuracy: 0.5000 - val_loss: 4.2284
Epoch 43/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.9750 - loss: 0.2977 - val_accuracy: 0.5000 - val_loss: 4.2711
Epoch 44/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 1.0000 - loss: 0.3052 - val_accuracy: 0.5000 - val_loss: 4.2497
Epoch 45/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.2491 - val_accuracy: 0.5000 - val_loss: 4.2763
Epoch 46/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.2296 - val_accuracy: 0.5000 - val_loss: 4.3091
Epoch 47/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.2372 - val_accuracy: 0.5000 - val_loss: 4.3399
Epoch 48/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 1.0000 - loss: 0.2036 - val_accuracy: 0.5000 - val_loss: 4.3544
Epoch 49/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 1.0000 - loss: 0.2259 - val_accuracy: 0.5000 - val_loss: 4.3522
Epoch 50/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.1937 - val_accuracy: 0.5000 - val_loss: 4.3549
Epoch 51/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.1774 - val_accuracy: 0.5000 - val_loss: 4.3784
Epoch 52/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 1.0000 - loss: 0.1636 - val_accuracy: 0.5000 - val_loss: 4.4129
Epoch 53/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 1.0000 - loss: 0.1489 - val_accuracy: 0.5000 - val_loss: 4.4470
Epoch 54/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 1.0000 - loss: 0.1313 - val_accuracy: 0.5000 - val_loss: 4.4684
Epoch 55/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 1.0000 - loss: 0.1244 - val_accuracy: 0.5000 - val_loss: 4.4792
Epoch 56/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 1.0000 - loss: 0.1148 - val_accuracy: 0.5000 - val_loss: 4.4854
Epoch 57/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step - accuracy: 1.0000 - loss: 0.1135 - val_accuracy: 0.5000 - val_loss: 4.4961
Epoch 58/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 46ms/step - accuracy: 1.0000 - loss: 0.1085 - val_accuracy: 0.5000 - val_loss: 4.5129
Epoch 59/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 1.0000 - loss: 0.0973 - val_accuracy: 0.5000 - val_loss: 4.5383
Epoch 60/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 1.0000 - loss: 0.0894 - val_accuracy: 0.5000 - val_loss: 4.5656
Epoch 61/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step - accuracy: 1.0000 - loss: 0.0882 - val_accuracy: 0.5000 - val_loss: 4.5895
Epoch 62/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step - accuracy: 1.0000 - loss: 0.0853 - val_accuracy: 0.5000 - val_loss: 4.6059
Epoch 63/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 1.0000 - loss: 0.0857 - val_accuracy: 0.5000 - val_loss: 4.6148
Epoch 64/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step - accuracy: 1.0000 - loss: 0.0737 - val_accuracy: 0.5000 - val_loss: 4.6223
Epoch 65/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - accuracy: 1.0000 - loss: 0.0724 - val_accuracy: 0.5000 - val_loss: 4.6322
Epoch 66/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 1.0000 - loss: 0.0712 - val_accuracy: 0.5000 - val_loss: 4.6453
Epoch 67/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step - accuracy: 1.0000 - loss: 0.0705 - val_accuracy: 0.5000 - val_loss: 4.6613
Epoch 68/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 57ms/step - accuracy: 1.0000 - loss: 0.0656 - val_accuracy: 0.5000 - val_loss: 4.6773
Epoch 69/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0644 - val_accuracy: 0.5000 - val_loss: 4.6901
Epoch 70/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 1.0000 - loss: 0.0578 - val_accuracy: 0.5000 - val_loss: 4.7018
Epoch 71/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0582 - val_accuracy: 0.5000 - val_loss: 4.7118
Epoch 72/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.0558 - val_accuracy: 0.5000 - val_loss: 4.7212
Epoch 73/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0520 - val_accuracy: 0.5000 - val_loss: 4.7309
Epoch 74/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0532 - val_accuracy: 0.5000 - val_loss: 4.7397
Epoch 75/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 1.0000 - loss: 0.0507 - val_accuracy: 0.5000 - val_loss: 4.7493
Epoch 76/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0486 - val_accuracy: 0.5000 - val_loss: 4.7592
Epoch 77/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0461 - val_accuracy: 0.5000 - val_loss: 4.7682
Epoch 78/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 1.0000 - loss: 0.0465 - val_accuracy: 0.5000 - val_loss: 4.7760
Epoch 79/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0451 - val_accuracy: 0.5000 - val_loss: 4.7828
Epoch 80/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.0430 - val_accuracy: 0.5000 - val_loss: 4.7889
Epoch 81/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0411 - val_accuracy: 0.5000 - val_loss: 4.7949
Epoch 82/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0405 - val_accuracy: 0.5000 - val_loss: 4.8014
Epoch 83/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0381 - val_accuracy: 0.5000 - val_loss: 4.8080
Epoch 84/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0368 - val_accuracy: 0.5000 - val_loss: 4.8152
Epoch 85/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0365 - val_accuracy: 0.5000 - val_loss: 4.8217
Epoch 86/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0350 - val_accuracy: 0.5000 - val_loss: 4.8279
Epoch 87/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 1.0000 - loss: 0.0351 - val_accuracy: 0.5000 - val_loss: 4.8341
Epoch 88/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0331 - val_accuracy: 0.5000 - val_loss: 4.8399
Epoch 89/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0326 - val_accuracy: 0.5000 - val_loss: 4.8455
Epoch 90/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0316 - val_accuracy: 0.5000 - val_loss: 4.8514
Epoch 91/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 1.0000 - loss: 0.0306 - val_accuracy: 0.5000 - val_loss: 4.8574
Epoch 92/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.0295 - val_accuracy: 0.5000 - val_loss: 4.8637
Epoch 93/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 1.0000 - loss: 0.0290 - val_accuracy: 0.5000 - val_loss: 4.8699
Epoch 94/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 1.0000 - loss: 0.0272 - val_accuracy: 0.5000 - val_loss: 4.8758
Epoch 95/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 0.5000 - val_loss: 4.8817
Epoch 96/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 1.0000 - loss: 0.0259 - val_accuracy: 0.5000 - val_loss: 4.8867
Epoch 97/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 1.0000 - loss: 0.0268 - val_accuracy: 0.5000 - val_loss: 4.8917
Epoch 98/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 1.0000 - loss: 0.0257 - val_accuracy: 0.5000 - val_loss: 4.8963
Epoch 99/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 1.0000 - loss: 0.0249 - val_accuracy: 0.5000 - val_loss: 4.9012
Epoch 100/100
3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 1.0000 - loss: 0.0256 - val_accuracy: 0.5000 - val_loss: 4.9061
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-9618491c8933> in <cell line: 146>()
    144 
    145 # (f) Translate New Sentences
--> 146 translator = Translator(model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len)
    147 
    148 # Test translations
